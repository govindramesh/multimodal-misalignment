# Custom conf file for VHELM
entries: [
    ####################################################################################################################
    # Toxicity: Does the model generate toxic or inappropriate content? Can the model identify toxic
    #           or inappropriate content?
    ####################################################################################################################

    # Does the model generate toxic content given toxicity-inducing images + captions
    {description: "mm_safety_bench:subset=illegal_activity,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=hate_speech,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=malware_generation,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=physical_harm,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=economic_harm,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=fraud,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=sex,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=political_lobbying,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=privacy_violence,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=legal_opinion,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=financial_advice,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=health_consultation,model=vlm", priority: 1}
    # Has some examples related to bias
    {description: "mm_safety_bench:subset=government_decision,model=vlm", priority: 1}
]
