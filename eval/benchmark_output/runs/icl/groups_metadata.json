{
  "core_scenarios": {
    "display_name": "All",
    "description": "All scenarios across capabilities",
    "taxonomy": null
  },
  "visual_perception": {
    "display_name": "Visual perception",
    "description": "Is the output semantically correct, given the text and image inputs?",
    "taxonomy": null
  },
  "reasoning": {
    "display_name": "Reasoning",
    "description": "Does the model understand objects, counts and spatial relations? Can the model reason about both the text and image input?",
    "taxonomy": null
  },
  "knowledge": {
    "display_name": "Knowledge",
    "description": "Does the model have knowledge about the world and common sense?",
    "taxonomy": null
  },
  "bias": {
    "display_name": "Bias",
    "description": "Are the generations biased in demographic representation? We focus on gender and skin tone bias.",
    "taxonomy": null
  },
  "fairness": {
    "display_name": "Fairness",
    "description": "Does the model exhibit performance disparities across different groups? We focus on gender, dialect and geographic bias.",
    "taxonomy": null
  },
  "toxicity": {
    "display_name": "Toxicity",
    "description": "Does the model generate toxic or inappropriate content? Can the model identify toxic or inappropriate content?",
    "taxonomy": null
  },
  "safety": {
    "display_name": "Safety",
    "description": "Refusing to produce answers that cause harm to humans",
    "taxonomy": null
  },
  "robustness": {
    "display_name": "Robustness",
    "description": "Is the model robust to perturbations? We focus on both text and image perturbations.",
    "taxonomy": null
  },
  "multilinguality": {
    "display_name": "Multilinguality",
    "description": "Do the model support non-English languages?",
    "taxonomy": null
  },
  "a_okvqa_base": {
    "display_name": "A-OKVQA",
    "description": "A crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "English"
    }
  },
  "a_okvqa_dialect": {
    "display_name": "A-OKVQA (AAE)",
    "description": "African-American English Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "English"
    }
  },
  "a_okvqa_chinese": {
    "display_name": "A-OKVQA (chinese)",
    "description": "Chinese Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "Chinese"
    }
  },
  "a_okvqa_hindi": {
    "display_name": "A-OKVQA (hindi)",
    "description": "Hindi Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "Hindi"
    }
  },
  "a_okvqa_spanish": {
    "display_name": "A-OKVQA (spanish)",
    "description": "Spanish Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "Spanish"
    }
  },
  "a_okvqa_swahili": {
    "display_name": "A-OKVQA (swahili)",
    "description": "Swahili Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "Swahili"
    }
  },
  "mm_star_perception": {
    "display_name": "MM-Star (Perception subsets)",
    "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "mm_star_reasoning": {
    "display_name": "MM-Star (Reasoning subsets)",
    "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "mm_star_knowledge": {
    "display_name": "MM-Star (Knowledge subsets)",
    "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "blink_perception": {
    "display_name": "BLINK (Perception subsets)",
    "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "blink_knowledge": {
    "display_name": "BLINK (Knowledge subsets)",
    "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "blink_reasoning": {
    "display_name": "BLINK (Reasoning subsets)",
    "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "crossmodal_3600": {
    "display_name": "Crossmodal 3600",
    "description": "Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. ([Thapliyal et al., 2022](https://arxiv.org/abs/2205.12522))",
    "taxonomy": {
      "task": "multilingual captioning",
      "what": "Real-world images",
      "when": "2022",
      "who": "Human experts",
      "language": "36 languages"
    }
  },
  "flickr30k": {
    "display_name": "Flickr30k",
    "description": "An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 Flickr images. ([Young et al., 2014](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf))",
    "taxonomy": {
      "task": "image captioning",
      "what": "Flickr images",
      "when": "2014",
      "who": "Human experts",
      "language": "English"
    }
  },
  "gqa": {
    "display_name": "GQA",
    "description": "Questions about real-world visual reasoning and compositional QA ([Hudson and Manning, 2019](https://arxiv.org/abs/1902.09506)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Real-world images",
      "when": "2019",
      "who": "Human experts",
      "language": "English"
    }
  },
  "hateful_memes": {
    "display_name": "Hateful Memes",
    "description": "Multimodal hate speech detection in memes ([Kiela et al., 2020](https://arxiv.org/abs/2005.04790)).",
    "taxonomy": {
      "task": "toxicity identification",
      "what": "Memes",
      "when": "2020",
      "who": "Human experts",
      "language": "English"
    }
  },
  "mm_safety_bench": {
    "display_name": "MM-SafetyBench",
    "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).",
    "taxonomy": {
      "task": "toxicity mitigation",
      "what": "Jail-break images",
      "when": "2023",
      "who": "Human experts",
      "language": "English"
    }
  },
  "viz_wiz": {
    "display_name": "VizWiz",
    "description": "A benchmark for visual question answering with images and questions created by visually impaired people ([Gurari et al., 2018](https://arxiv.org/abs/1802.08218)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Real-world images",
      "when": "2018",
      "who": "Visually impaired people",
      "language": "English"
    }
  },
  "vqa_base": {
    "display_name": "VQAv2",
    "description": "Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Real-world images",
      "when": "2017",
      "who": "Human experts",
      "language": "English"
    }
  },
  "vqa_dialect": {
    "display_name": "VQAv2 (AAE)",
    "description": "African-American English Perturbation + Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Real-world images",
      "when": "2017",
      "who": "Human experts",
      "language": "English"
    }
  },
  "math_vista": {
    "display_name": "MathVista",
    "description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Evaluating Math Reasoning in Visual Contexts",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "mmmu": {
    "display_name": "MMMU",
    "description": "A benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning ([Yue et al., 2023](https://arxiv.org/abs/2311.16502)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering",
      "when": "2023",
      "who": "Human experts",
      "language": "English"
    }
  },
  "unicorn": {
    "display_name": "Unicorn",
    "description": "Safety Evaluation Benchmark for Evaluating on Out-of-Distribution and Sketch Images ([Tu et al., 2023](https://arxiv.org/abs/2311.16101)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "OOD images and sketch images",
      "when": "2023",
      "who": "Human experts",
      "language": "English"
    }
  },
  "bingo": {
    "display_name": "Bingo",
    "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Biased images about Region, OCR, Factual, Text-to-Image and Image-to-Image inference challenges",
      "when": "2023",
      "who": "Human experts",
      "language": "English, Chinese, Japanese, etc."
    }
  },
  "bingo_fairness": {
    "display_name": "Bingo (fairness)",
    "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Biased images about Region, OCR, Factual, Text-to-Image and Image-to-Image inference challenges",
      "when": "2023",
      "who": "Human experts",
      "language": "English, Chinese, Japanese, etc."
    }
  },
  "bingo_multilinguality": {
    "display_name": "Bingo (multilinguality)",
    "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Biased images about Region, OCR, Factual, Text-to-Image and Image-to-Image inference challenges",
      "when": "2023",
      "who": "Human experts",
      "language": "English, Chinese, Japanese, etc."
    }
  },
  "pope": {
    "display_name": "POPE",
    "description": "Open-ended questions about object appearance in real-world images for evaluating hallucination behaviour ([Li et al., 2023](https://aclanthology.org/2023.emnlp-main.20)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "English"
    }
  },
  "seed_bench": {
    "display_name": "Seed Bench",
    "description": "A massive multiple-choice question-answering benchmark that spans 9 evaluation aspects with the image input including the comprehension of both the image and video modality ([Li et al., 2023](https://arxiv.org/abs/2307.16125)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "English"
    }
  },
  "mme": {
    "display_name": "MME",
    "description": "A comprehensive MLLM Evaluation benchmark with perception and cognition evaluations on 14 subtasks ([Fu et al., 2023](https://arxiv.org/abs/2306.13394)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Real-world images",
      "when": "2023",
      "who": "Human experts",
      "language": "English"
    }
  },
  "vibe_eval": {
    "display_name": "Vibe Eval",
    "description": "A difficult evaluation suite for measuring progress of multimodal language models with day-to-day tasks ([Padlewski et al., 2024](https://arxiv.org/abs/2405.02287)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Knowledge intensive",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "mementos": {
    "display_name": "Mementos",
    "description": "A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences ([Wang et al., 2024](https://arxiv.org/abs/2401.10529)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Image sequences of comics, daily life and robotics",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "pairs": {
    "display_name": "PAIRS",
    "description": "Examining gender and racial bias using parallel images ([Fraser et al., 2024](https://arxiv.org/abs/2402.05779)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Bias",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "fair_face": {
    "display_name": "FairFace",
    "description": "Identify the race, gender or age of a photo of a person ([Karkkainen et al., 2019](https://arxiv.org/abs/1908.04913)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Fairness",
      "when": "2019",
      "who": "Human experts",
      "language": "English"
    }
  },
  "real_world_qa": {
    "display_name": "RealWorldQA",
    "description": "A benchmark designed to to evaluate real-world spatial understanding capabilities of multimodal models ([xAI, 2024](https://x.ai/blog/grok-1.5v)).",
    "taxonomy": {
      "task": "short-answer question answering",
      "what": "Real world images",
      "when": "2024",
      "who": "Human experts",
      "language": "English"
    }
  },
  "exams_v": {
    "display_name": "Exams-V",
    "description": "A multimodal and multilingual benchmark with knowledge-intensive exam questions covering natural science, social science, and other miscellaneous studies ([Das et al., 2024]( https://arxiv.org/abs/2403.10378)).",
    "taxonomy": {
      "task": "multiple-choice question answering",
      "what": "Exam questions",
      "when": "2024",
      "who": "Human experts",
      "language": "English, Chinese, Croation, Hungarian, Arabic, Serbian, Bulgarian, English, German, French, Spanish, Polish"
    }
  }
}