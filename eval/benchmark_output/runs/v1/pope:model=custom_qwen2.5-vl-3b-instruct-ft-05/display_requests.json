[
  {
    "instance_id": "id4424",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000100215.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a bed in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id1726",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000473199.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a potted plant in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id7895",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000515820.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a dining table in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id7586",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000198312.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a couch in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id1986",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000543393.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a fire hydrant in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id3018",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000211674.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a person in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id1467",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000391735.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a sink in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id7968",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000044801.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a traffic light in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id154",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000297078.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a skis in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  },
  {
    "instance_id": "id2315",
    "train_trial_index": 0,
    "request": {
      "model_deployment": "huggingface/qwen2.5-vl-3b-instruct-ft-05",
      "model": "custom/qwen2.5-vl-3b-instruct-ft-05",
      "embedding": false,
      "prompt": "",
      "temperature": 0.0,
      "num_completions": 1,
      "top_k_per_token": 1,
      "max_tokens": 1,
      "stop_sequences": [],
      "echo_prompt": false,
      "top_p": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "multimodal_prompt": {
        "media_objects": [
          {
            "content_type": "text/plain",
            "text": "Answer the multiple choice question by just giving the letter of the correct answer."
          },
          {
            "content_type": "text/plain",
            "text": "\n"
          },
          {
            "content_type": "image/jpeg",
            "location": "benchmark_output/scenarios/pope/images/COCO_val2014_000000042685.jpg"
          },
          {
            "content_type": "text/plain",
            "text": "Is there a bottle in the image?\nA. Yes\nB. No\nAnswer:"
          }
        ]
      }
    }
  }
]