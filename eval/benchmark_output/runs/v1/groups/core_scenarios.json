[
  {
    "title": "Accuracy",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Mean win rate",
        "description": "How many models this model outperforms on average (over columns).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {}
      },
      {
        "value": "VQAv2 - PEM",
        "description": "Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "VQAv2"
        }
      },
      {
        "value": "VizWiz - PEM",
        "description": "A benchmark for visual question answering with images and questions created by visually impaired people ([Gurari et al., 2018](https://arxiv.org/abs/1802.08218)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "VizWiz"
        }
      },
      {
        "value": "Flickr30k - Prometheus Vision rating",
        "description": "An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 Flickr images. ([Young et al., 2014](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf))\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "Prometheus Vision rating",
          "run_group": "Flickr30k"
        }
      },
      {
        "value": "POPE - PEM",
        "description": "Open-ended questions about object appearance in real-world images for evaluating hallucination behaviour ([Li et al., 2023](https://aclanthology.org/2023.emnlp-main.20)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "POPE"
        }
      },
      {
        "value": "MM-Star (Perception subsets) - PEM",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "MM-Star (Perception subsets)"
        }
      },
      {
        "value": "BLINK (Perception subsets) - PEM",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "BLINK (Perception subsets)"
        }
      },
      {
        "value": "MMMU - PEM",
        "description": "A benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning ([Yue et al., 2023](https://arxiv.org/abs/2311.16502)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "MMMU"
        }
      },
      {
        "value": "Exams-V - PEM",
        "description": "A multimodal and multilingual benchmark with knowledge-intensive exam questions covering natural science, social science, and other miscellaneous studies ([Das et al., 2024]( https://arxiv.org/abs/2403.10378)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "Exams-V"
        }
      },
      {
        "value": "GQA - PEM",
        "description": "Questions about real-world visual reasoning and compositional QA ([Hudson and Manning, 2019](https://arxiv.org/abs/1902.09506)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "GQA"
        }
      },
      {
        "value": "MathVista - EM",
        "description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MathVista"
        }
      },
      {
        "value": "Seed Bench - PEM",
        "description": "A massive multiple-choice question-answering benchmark that spans 9 evaluation aspects with the image input including the comprehension of both the image and video modality ([Li et al., 2023](https://arxiv.org/abs/2307.16125)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "Seed Bench"
        }
      },
      {
        "value": "Mementos - Prometheus Vision rating",
        "description": "A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences ([Wang et al., 2024](https://arxiv.org/abs/2401.10529)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "Prometheus Vision rating",
          "run_group": "Mementos"
        }
      },
      {
        "value": "RealWorldQA - EM",
        "description": "A benchmark designed to to evaluate real-world spatial understanding capabilities of multimodal models ([xAI, 2024](https://x.ai/blog/grok-1.5v)).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "RealWorldQA"
        }
      },
      {
        "value": "MM-Star (Reasoning subsets) - PEM",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "MM-Star (Reasoning subsets)"
        }
      },
      {
        "value": "BLINK (Reasoning subsets) - PEM",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "BLINK (Reasoning subsets)"
        }
      },
      {
        "value": "A-OKVQA - PEM",
        "description": "A crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "A-OKVQA"
        }
      },
      {
        "value": "MME - PEM",
        "description": "A comprehensive MLLM Evaluation benchmark with perception and cognition evaluations on 14 subtasks ([Fu et al., 2023](https://arxiv.org/abs/2306.13394)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "MME"
        }
      },
      {
        "value": "Vibe Eval - Prometheus Vision rating",
        "description": "A difficult evaluation suite for measuring progress of multimodal language models with day-to-day tasks ([Padlewski et al., 2024](https://arxiv.org/abs/2405.02287)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "Prometheus Vision rating",
          "run_group": "Vibe Eval"
        }
      },
      {
        "value": "MM-Star (Knowledge subsets) - PEM",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "MM-Star (Knowledge subsets)"
        }
      },
      {
        "value": "BLINK (Knowledge subsets) - PEM",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "BLINK (Knowledge subsets)"
        }
      },
      {
        "value": "PAIRS - PEM",
        "description": "Examining gender and racial bias using parallel images ([Fraser et al., 2024](https://arxiv.org/abs/2402.05779)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "PAIRS"
        }
      },
      {
        "value": "Crossmodal 3600 - Prometheus Vision rating",
        "description": "Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. ([Thapliyal et al., 2022](https://arxiv.org/abs/2205.12522))\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "Prometheus Vision rating",
          "run_group": "Crossmodal 3600"
        }
      },
      {
        "value": "FairFace - PEM",
        "description": "Identify the race, gender or age of a photo of a person ([Karkkainen et al., 2019](https://arxiv.org/abs/1908.04913)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "FairFace"
        }
      },
      {
        "value": "Bingo (fairness) - Prometheus Vision rating",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "Prometheus Vision rating",
          "run_group": "Bingo (fairness)"
        }
      },
      {
        "value": "MM-SafetyBench - Toxic fraction",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\nToxic fraction: Fraction of model outputs that are toxic (based on the PerspectiveAPI toxicity classifier).",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Toxic fraction",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "Hateful Memes - PEM",
        "description": "Multimodal hate speech detection in memes ([Kiela et al., 2020](https://arxiv.org/abs/2005.04790)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "Hateful Memes"
        }
      },
      {
        "value": "Unicorn - PEM",
        "description": "Safety Evaluation Benchmark for Evaluating on Out-of-Distribution and Sketch Images ([Tu et al., 2023](https://arxiv.org/abs/2311.16101)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "Unicorn"
        }
      },
      {
        "value": "Bingo - Prometheus Vision rating",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "Prometheus Vision rating",
          "run_group": "Bingo"
        }
      },
      {
        "value": "Bingo (multilinguality) - Prometheus Vision rating",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "Prometheus Vision rating",
          "run_group": "Bingo (multilinguality)"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.625,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.42870370370370364,
          "description": "min=0.2, mean=0.429, max=0.6, sum=5.144 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.611111111111111,
          "description": "min=0.1, mean=0.611, max=1, sum=11.0 (18)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.9,
          "description": "min=0.9, mean=0.9, max=0.9, sum=1.8 (2)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.375,
          "markdown": false
        },
        {
          "value": 0.4,
          "description": "min=0.4, mean=0.4, max=0.4, sum=0.4 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,groups=vqa_base"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.4574074074074073,
          "description": "min=0.2, mean=0.457, max=0.6, sum=5.489 (12)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.5888888888888888,
          "description": "min=0, mean=0.589, max=1, sum=10.6 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.8500000000000001,
          "description": "min=0.8, mean=0.85, max=0.9, sum=1.7 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5416666666666666,
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.41944444444444445,
          "description": "min=0.2, mean=0.419, max=0.7, sum=5.033 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.611111111111111,
          "description": "min=0, mean=0.611, max=1, sum=11.0 (18)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.9,
          "description": "min=0.9, mean=0.9, max=0.9, sum=1.8 (2)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.4583333333333333,
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.41111111111111104,
          "description": "min=0.2, mean=0.411, max=0.6, sum=4.933 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.611111111111111,
          "description": "min=0.1, mean=0.611, max=1, sum=11.0 (18)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 0.9,
          "description": "min=0.9, mean=0.9, max=0.9, sum=1.8 (2)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/core_scenarios_accuracy.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/core_scenarios_accuracy.json"
      }
    ],
    "name": "accuracy"
  },
  {
    "title": "General information",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Mean win rate",
        "description": "How many models this model outperforms on average (over columns).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {}
      },
      {
        "value": "VQAv2 - # eval",
        "description": "Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "VQAv2"
        }
      },
      {
        "value": "VQAv2 - # train",
        "description": "Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "VQAv2"
        }
      },
      {
        "value": "VQAv2 - truncated",
        "description": "Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "VQAv2"
        }
      },
      {
        "value": "VQAv2 - # prompt tokens",
        "description": "Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "VQAv2"
        }
      },
      {
        "value": "VQAv2 - # output tokens",
        "description": "Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "VQAv2"
        }
      },
      {
        "value": "VizWiz - # eval",
        "description": "A benchmark for visual question answering with images and questions created by visually impaired people ([Gurari et al., 2018](https://arxiv.org/abs/1802.08218)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "VizWiz"
        }
      },
      {
        "value": "VizWiz - # train",
        "description": "A benchmark for visual question answering with images and questions created by visually impaired people ([Gurari et al., 2018](https://arxiv.org/abs/1802.08218)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "VizWiz"
        }
      },
      {
        "value": "VizWiz - truncated",
        "description": "A benchmark for visual question answering with images and questions created by visually impaired people ([Gurari et al., 2018](https://arxiv.org/abs/1802.08218)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "VizWiz"
        }
      },
      {
        "value": "VizWiz - # prompt tokens",
        "description": "A benchmark for visual question answering with images and questions created by visually impaired people ([Gurari et al., 2018](https://arxiv.org/abs/1802.08218)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "VizWiz"
        }
      },
      {
        "value": "VizWiz - # output tokens",
        "description": "A benchmark for visual question answering with images and questions created by visually impaired people ([Gurari et al., 2018](https://arxiv.org/abs/1802.08218)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "VizWiz"
        }
      },
      {
        "value": "Flickr30k - # eval",
        "description": "An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 Flickr images. ([Young et al., 2014](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf))\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Flickr30k"
        }
      },
      {
        "value": "Flickr30k - # train",
        "description": "An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 Flickr images. ([Young et al., 2014](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf))\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Flickr30k"
        }
      },
      {
        "value": "Flickr30k - truncated",
        "description": "An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 Flickr images. ([Young et al., 2014](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf))\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Flickr30k"
        }
      },
      {
        "value": "Flickr30k - # prompt tokens",
        "description": "An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 Flickr images. ([Young et al., 2014](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf))\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Flickr30k"
        }
      },
      {
        "value": "Flickr30k - # output tokens",
        "description": "An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 Flickr images. ([Young et al., 2014](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf))\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Flickr30k"
        }
      },
      {
        "value": "POPE - # eval",
        "description": "Open-ended questions about object appearance in real-world images for evaluating hallucination behaviour ([Li et al., 2023](https://aclanthology.org/2023.emnlp-main.20)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "POPE"
        }
      },
      {
        "value": "POPE - # train",
        "description": "Open-ended questions about object appearance in real-world images for evaluating hallucination behaviour ([Li et al., 2023](https://aclanthology.org/2023.emnlp-main.20)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "POPE"
        }
      },
      {
        "value": "POPE - truncated",
        "description": "Open-ended questions about object appearance in real-world images for evaluating hallucination behaviour ([Li et al., 2023](https://aclanthology.org/2023.emnlp-main.20)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "POPE"
        }
      },
      {
        "value": "POPE - # prompt tokens",
        "description": "Open-ended questions about object appearance in real-world images for evaluating hallucination behaviour ([Li et al., 2023](https://aclanthology.org/2023.emnlp-main.20)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "POPE"
        }
      },
      {
        "value": "POPE - # output tokens",
        "description": "Open-ended questions about object appearance in real-world images for evaluating hallucination behaviour ([Li et al., 2023](https://aclanthology.org/2023.emnlp-main.20)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "POPE"
        }
      },
      {
        "value": "MM-Star (Perception subsets) - # eval",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MM-Star (Perception subsets)"
        }
      },
      {
        "value": "MM-Star (Perception subsets) - # train",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MM-Star (Perception subsets)"
        }
      },
      {
        "value": "MM-Star (Perception subsets) - truncated",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MM-Star (Perception subsets)"
        }
      },
      {
        "value": "MM-Star (Perception subsets) - # prompt tokens",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MM-Star (Perception subsets)"
        }
      },
      {
        "value": "MM-Star (Perception subsets) - # output tokens",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MM-Star (Perception subsets)"
        }
      },
      {
        "value": "BLINK (Perception subsets) - # eval",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "BLINK (Perception subsets)"
        }
      },
      {
        "value": "BLINK (Perception subsets) - # train",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "BLINK (Perception subsets)"
        }
      },
      {
        "value": "BLINK (Perception subsets) - truncated",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "BLINK (Perception subsets)"
        }
      },
      {
        "value": "BLINK (Perception subsets) - # prompt tokens",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "BLINK (Perception subsets)"
        }
      },
      {
        "value": "BLINK (Perception subsets) - # output tokens",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "BLINK (Perception subsets)"
        }
      },
      {
        "value": "MMMU - # eval",
        "description": "A benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning ([Yue et al., 2023](https://arxiv.org/abs/2311.16502)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMMU"
        }
      },
      {
        "value": "MMMU - # train",
        "description": "A benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning ([Yue et al., 2023](https://arxiv.org/abs/2311.16502)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMMU"
        }
      },
      {
        "value": "MMMU - truncated",
        "description": "A benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning ([Yue et al., 2023](https://arxiv.org/abs/2311.16502)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMMU"
        }
      },
      {
        "value": "MMMU - # prompt tokens",
        "description": "A benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning ([Yue et al., 2023](https://arxiv.org/abs/2311.16502)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMMU"
        }
      },
      {
        "value": "MMMU - # output tokens",
        "description": "A benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning ([Yue et al., 2023](https://arxiv.org/abs/2311.16502)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMMU"
        }
      },
      {
        "value": "Exams-V - # eval",
        "description": "A multimodal and multilingual benchmark with knowledge-intensive exam questions covering natural science, social science, and other miscellaneous studies ([Das et al., 2024]( https://arxiv.org/abs/2403.10378)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Exams-V"
        }
      },
      {
        "value": "Exams-V - # train",
        "description": "A multimodal and multilingual benchmark with knowledge-intensive exam questions covering natural science, social science, and other miscellaneous studies ([Das et al., 2024]( https://arxiv.org/abs/2403.10378)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Exams-V"
        }
      },
      {
        "value": "Exams-V - truncated",
        "description": "A multimodal and multilingual benchmark with knowledge-intensive exam questions covering natural science, social science, and other miscellaneous studies ([Das et al., 2024]( https://arxiv.org/abs/2403.10378)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Exams-V"
        }
      },
      {
        "value": "Exams-V - # prompt tokens",
        "description": "A multimodal and multilingual benchmark with knowledge-intensive exam questions covering natural science, social science, and other miscellaneous studies ([Das et al., 2024]( https://arxiv.org/abs/2403.10378)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Exams-V"
        }
      },
      {
        "value": "Exams-V - # output tokens",
        "description": "A multimodal and multilingual benchmark with knowledge-intensive exam questions covering natural science, social science, and other miscellaneous studies ([Das et al., 2024]( https://arxiv.org/abs/2403.10378)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Exams-V"
        }
      },
      {
        "value": "GQA - # eval",
        "description": "Questions about real-world visual reasoning and compositional QA ([Hudson and Manning, 2019](https://arxiv.org/abs/1902.09506)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "GQA"
        }
      },
      {
        "value": "GQA - # train",
        "description": "Questions about real-world visual reasoning and compositional QA ([Hudson and Manning, 2019](https://arxiv.org/abs/1902.09506)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "GQA"
        }
      },
      {
        "value": "GQA - truncated",
        "description": "Questions about real-world visual reasoning and compositional QA ([Hudson and Manning, 2019](https://arxiv.org/abs/1902.09506)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "GQA"
        }
      },
      {
        "value": "GQA - # prompt tokens",
        "description": "Questions about real-world visual reasoning and compositional QA ([Hudson and Manning, 2019](https://arxiv.org/abs/1902.09506)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "GQA"
        }
      },
      {
        "value": "GQA - # output tokens",
        "description": "Questions about real-world visual reasoning and compositional QA ([Hudson and Manning, 2019](https://arxiv.org/abs/1902.09506)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "GQA"
        }
      },
      {
        "value": "MathVista - # eval",
        "description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MathVista"
        }
      },
      {
        "value": "MathVista - # train",
        "description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MathVista"
        }
      },
      {
        "value": "MathVista - truncated",
        "description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MathVista"
        }
      },
      {
        "value": "MathVista - # prompt tokens",
        "description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MathVista"
        }
      },
      {
        "value": "MathVista - # output tokens",
        "description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MathVista"
        }
      },
      {
        "value": "Seed Bench - # eval",
        "description": "A massive multiple-choice question-answering benchmark that spans 9 evaluation aspects with the image input including the comprehension of both the image and video modality ([Li et al., 2023](https://arxiv.org/abs/2307.16125)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Seed Bench"
        }
      },
      {
        "value": "Seed Bench - # train",
        "description": "A massive multiple-choice question-answering benchmark that spans 9 evaluation aspects with the image input including the comprehension of both the image and video modality ([Li et al., 2023](https://arxiv.org/abs/2307.16125)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Seed Bench"
        }
      },
      {
        "value": "Seed Bench - truncated",
        "description": "A massive multiple-choice question-answering benchmark that spans 9 evaluation aspects with the image input including the comprehension of both the image and video modality ([Li et al., 2023](https://arxiv.org/abs/2307.16125)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Seed Bench"
        }
      },
      {
        "value": "Seed Bench - # prompt tokens",
        "description": "A massive multiple-choice question-answering benchmark that spans 9 evaluation aspects with the image input including the comprehension of both the image and video modality ([Li et al., 2023](https://arxiv.org/abs/2307.16125)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Seed Bench"
        }
      },
      {
        "value": "Seed Bench - # output tokens",
        "description": "A massive multiple-choice question-answering benchmark that spans 9 evaluation aspects with the image input including the comprehension of both the image and video modality ([Li et al., 2023](https://arxiv.org/abs/2307.16125)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Seed Bench"
        }
      },
      {
        "value": "Mementos - # eval",
        "description": "A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences ([Wang et al., 2024](https://arxiv.org/abs/2401.10529)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Mementos"
        }
      },
      {
        "value": "Mementos - # train",
        "description": "A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences ([Wang et al., 2024](https://arxiv.org/abs/2401.10529)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Mementos"
        }
      },
      {
        "value": "Mementos - truncated",
        "description": "A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences ([Wang et al., 2024](https://arxiv.org/abs/2401.10529)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Mementos"
        }
      },
      {
        "value": "Mementos - # prompt tokens",
        "description": "A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences ([Wang et al., 2024](https://arxiv.org/abs/2401.10529)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Mementos"
        }
      },
      {
        "value": "Mementos - # output tokens",
        "description": "A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences ([Wang et al., 2024](https://arxiv.org/abs/2401.10529)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Mementos"
        }
      },
      {
        "value": "RealWorldQA - # eval",
        "description": "A benchmark designed to to evaluate real-world spatial understanding capabilities of multimodal models ([xAI, 2024](https://x.ai/blog/grok-1.5v)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "RealWorldQA"
        }
      },
      {
        "value": "RealWorldQA - # train",
        "description": "A benchmark designed to to evaluate real-world spatial understanding capabilities of multimodal models ([xAI, 2024](https://x.ai/blog/grok-1.5v)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "RealWorldQA"
        }
      },
      {
        "value": "RealWorldQA - truncated",
        "description": "A benchmark designed to to evaluate real-world spatial understanding capabilities of multimodal models ([xAI, 2024](https://x.ai/blog/grok-1.5v)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "RealWorldQA"
        }
      },
      {
        "value": "RealWorldQA - # prompt tokens",
        "description": "A benchmark designed to to evaluate real-world spatial understanding capabilities of multimodal models ([xAI, 2024](https://x.ai/blog/grok-1.5v)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "RealWorldQA"
        }
      },
      {
        "value": "RealWorldQA - # output tokens",
        "description": "A benchmark designed to to evaluate real-world spatial understanding capabilities of multimodal models ([xAI, 2024](https://x.ai/blog/grok-1.5v)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "RealWorldQA"
        }
      },
      {
        "value": "MM-Star (Reasoning subsets) - # eval",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MM-Star (Reasoning subsets)"
        }
      },
      {
        "value": "MM-Star (Reasoning subsets) - # train",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MM-Star (Reasoning subsets)"
        }
      },
      {
        "value": "MM-Star (Reasoning subsets) - truncated",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MM-Star (Reasoning subsets)"
        }
      },
      {
        "value": "MM-Star (Reasoning subsets) - # prompt tokens",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MM-Star (Reasoning subsets)"
        }
      },
      {
        "value": "MM-Star (Reasoning subsets) - # output tokens",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MM-Star (Reasoning subsets)"
        }
      },
      {
        "value": "BLINK (Reasoning subsets) - # eval",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "BLINK (Reasoning subsets)"
        }
      },
      {
        "value": "BLINK (Reasoning subsets) - # train",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "BLINK (Reasoning subsets)"
        }
      },
      {
        "value": "BLINK (Reasoning subsets) - truncated",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "BLINK (Reasoning subsets)"
        }
      },
      {
        "value": "BLINK (Reasoning subsets) - # prompt tokens",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "BLINK (Reasoning subsets)"
        }
      },
      {
        "value": "BLINK (Reasoning subsets) - # output tokens",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "BLINK (Reasoning subsets)"
        }
      },
      {
        "value": "A-OKVQA - # eval",
        "description": "A crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "A-OKVQA"
        }
      },
      {
        "value": "A-OKVQA - # train",
        "description": "A crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "A-OKVQA"
        }
      },
      {
        "value": "A-OKVQA - truncated",
        "description": "A crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "A-OKVQA"
        }
      },
      {
        "value": "A-OKVQA - # prompt tokens",
        "description": "A crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "A-OKVQA"
        }
      },
      {
        "value": "A-OKVQA - # output tokens",
        "description": "A crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "A-OKVQA"
        }
      },
      {
        "value": "MME - # eval",
        "description": "A comprehensive MLLM Evaluation benchmark with perception and cognition evaluations on 14 subtasks ([Fu et al., 2023](https://arxiv.org/abs/2306.13394)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MME"
        }
      },
      {
        "value": "MME - # train",
        "description": "A comprehensive MLLM Evaluation benchmark with perception and cognition evaluations on 14 subtasks ([Fu et al., 2023](https://arxiv.org/abs/2306.13394)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MME"
        }
      },
      {
        "value": "MME - truncated",
        "description": "A comprehensive MLLM Evaluation benchmark with perception and cognition evaluations on 14 subtasks ([Fu et al., 2023](https://arxiv.org/abs/2306.13394)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MME"
        }
      },
      {
        "value": "MME - # prompt tokens",
        "description": "A comprehensive MLLM Evaluation benchmark with perception and cognition evaluations on 14 subtasks ([Fu et al., 2023](https://arxiv.org/abs/2306.13394)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MME"
        }
      },
      {
        "value": "MME - # output tokens",
        "description": "A comprehensive MLLM Evaluation benchmark with perception and cognition evaluations on 14 subtasks ([Fu et al., 2023](https://arxiv.org/abs/2306.13394)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MME"
        }
      },
      {
        "value": "Vibe Eval - # eval",
        "description": "A difficult evaluation suite for measuring progress of multimodal language models with day-to-day tasks ([Padlewski et al., 2024](https://arxiv.org/abs/2405.02287)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Vibe Eval"
        }
      },
      {
        "value": "Vibe Eval - # train",
        "description": "A difficult evaluation suite for measuring progress of multimodal language models with day-to-day tasks ([Padlewski et al., 2024](https://arxiv.org/abs/2405.02287)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Vibe Eval"
        }
      },
      {
        "value": "Vibe Eval - truncated",
        "description": "A difficult evaluation suite for measuring progress of multimodal language models with day-to-day tasks ([Padlewski et al., 2024](https://arxiv.org/abs/2405.02287)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Vibe Eval"
        }
      },
      {
        "value": "Vibe Eval - # prompt tokens",
        "description": "A difficult evaluation suite for measuring progress of multimodal language models with day-to-day tasks ([Padlewski et al., 2024](https://arxiv.org/abs/2405.02287)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Vibe Eval"
        }
      },
      {
        "value": "Vibe Eval - # output tokens",
        "description": "A difficult evaluation suite for measuring progress of multimodal language models with day-to-day tasks ([Padlewski et al., 2024](https://arxiv.org/abs/2405.02287)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Vibe Eval"
        }
      },
      {
        "value": "MM-Star (Knowledge subsets) - # eval",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MM-Star (Knowledge subsets)"
        }
      },
      {
        "value": "MM-Star (Knowledge subsets) - # train",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MM-Star (Knowledge subsets)"
        }
      },
      {
        "value": "MM-Star (Knowledge subsets) - truncated",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MM-Star (Knowledge subsets)"
        }
      },
      {
        "value": "MM-Star (Knowledge subsets) - # prompt tokens",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MM-Star (Knowledge subsets)"
        }
      },
      {
        "value": "MM-Star (Knowledge subsets) - # output tokens",
        "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MM-Star (Knowledge subsets)"
        }
      },
      {
        "value": "BLINK (Knowledge subsets) - # eval",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "BLINK (Knowledge subsets)"
        }
      },
      {
        "value": "BLINK (Knowledge subsets) - # train",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "BLINK (Knowledge subsets)"
        }
      },
      {
        "value": "BLINK (Knowledge subsets) - truncated",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "BLINK (Knowledge subsets)"
        }
      },
      {
        "value": "BLINK (Knowledge subsets) - # prompt tokens",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "BLINK (Knowledge subsets)"
        }
      },
      {
        "value": "BLINK (Knowledge subsets) - # output tokens",
        "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "BLINK (Knowledge subsets)"
        }
      },
      {
        "value": "PAIRS - # eval",
        "description": "Examining gender and racial bias using parallel images ([Fraser et al., 2024](https://arxiv.org/abs/2402.05779)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "PAIRS"
        }
      },
      {
        "value": "PAIRS - # train",
        "description": "Examining gender and racial bias using parallel images ([Fraser et al., 2024](https://arxiv.org/abs/2402.05779)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "PAIRS"
        }
      },
      {
        "value": "PAIRS - truncated",
        "description": "Examining gender and racial bias using parallel images ([Fraser et al., 2024](https://arxiv.org/abs/2402.05779)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "PAIRS"
        }
      },
      {
        "value": "PAIRS - # prompt tokens",
        "description": "Examining gender and racial bias using parallel images ([Fraser et al., 2024](https://arxiv.org/abs/2402.05779)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "PAIRS"
        }
      },
      {
        "value": "PAIRS - # output tokens",
        "description": "Examining gender and racial bias using parallel images ([Fraser et al., 2024](https://arxiv.org/abs/2402.05779)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "PAIRS"
        }
      },
      {
        "value": "VQAv2 (AAE) - # eval",
        "description": "African-American English Perturbation + Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "VQAv2 (AAE)"
        }
      },
      {
        "value": "VQAv2 (AAE) - # train",
        "description": "African-American English Perturbation + Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "VQAv2 (AAE)"
        }
      },
      {
        "value": "VQAv2 (AAE) - truncated",
        "description": "African-American English Perturbation + Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "VQAv2 (AAE)"
        }
      },
      {
        "value": "VQAv2 (AAE) - # prompt tokens",
        "description": "African-American English Perturbation + Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "VQAv2 (AAE)"
        }
      },
      {
        "value": "VQAv2 (AAE) - # output tokens",
        "description": "African-American English Perturbation + Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "VQAv2 (AAE)"
        }
      },
      {
        "value": "A-OKVQA (AAE) - # eval",
        "description": "African-American English Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "A-OKVQA (AAE)"
        }
      },
      {
        "value": "A-OKVQA (AAE) - # train",
        "description": "African-American English Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "A-OKVQA (AAE)"
        }
      },
      {
        "value": "A-OKVQA (AAE) - truncated",
        "description": "African-American English Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "A-OKVQA (AAE)"
        }
      },
      {
        "value": "A-OKVQA (AAE) - # prompt tokens",
        "description": "African-American English Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "A-OKVQA (AAE)"
        }
      },
      {
        "value": "A-OKVQA (AAE) - # output tokens",
        "description": "African-American English Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "A-OKVQA (AAE)"
        }
      },
      {
        "value": "Crossmodal 3600 - # eval",
        "description": "Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. ([Thapliyal et al., 2022](https://arxiv.org/abs/2205.12522))\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Crossmodal 3600"
        }
      },
      {
        "value": "Crossmodal 3600 - # train",
        "description": "Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. ([Thapliyal et al., 2022](https://arxiv.org/abs/2205.12522))\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Crossmodal 3600"
        }
      },
      {
        "value": "Crossmodal 3600 - truncated",
        "description": "Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. ([Thapliyal et al., 2022](https://arxiv.org/abs/2205.12522))\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Crossmodal 3600"
        }
      },
      {
        "value": "Crossmodal 3600 - # prompt tokens",
        "description": "Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. ([Thapliyal et al., 2022](https://arxiv.org/abs/2205.12522))\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Crossmodal 3600"
        }
      },
      {
        "value": "Crossmodal 3600 - # output tokens",
        "description": "Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. ([Thapliyal et al., 2022](https://arxiv.org/abs/2205.12522))\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Crossmodal 3600"
        }
      },
      {
        "value": "FairFace - # eval",
        "description": "Identify the race, gender or age of a photo of a person ([Karkkainen et al., 2019](https://arxiv.org/abs/1908.04913)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "FairFace"
        }
      },
      {
        "value": "FairFace - # train",
        "description": "Identify the race, gender or age of a photo of a person ([Karkkainen et al., 2019](https://arxiv.org/abs/1908.04913)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "FairFace"
        }
      },
      {
        "value": "FairFace - truncated",
        "description": "Identify the race, gender or age of a photo of a person ([Karkkainen et al., 2019](https://arxiv.org/abs/1908.04913)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "FairFace"
        }
      },
      {
        "value": "FairFace - # prompt tokens",
        "description": "Identify the race, gender or age of a photo of a person ([Karkkainen et al., 2019](https://arxiv.org/abs/1908.04913)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "FairFace"
        }
      },
      {
        "value": "FairFace - # output tokens",
        "description": "Identify the race, gender or age of a photo of a person ([Karkkainen et al., 2019](https://arxiv.org/abs/1908.04913)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "FairFace"
        }
      },
      {
        "value": "Bingo (fairness) - # eval",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Bingo (fairness)"
        }
      },
      {
        "value": "Bingo (fairness) - # train",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Bingo (fairness)"
        }
      },
      {
        "value": "Bingo (fairness) - truncated",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Bingo (fairness)"
        }
      },
      {
        "value": "Bingo (fairness) - # prompt tokens",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Bingo (fairness)"
        }
      },
      {
        "value": "Bingo (fairness) - # output tokens",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Bingo (fairness)"
        }
      },
      {
        "value": "MM-SafetyBench - # eval",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "MM-SafetyBench - # train",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "MM-SafetyBench - truncated",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "MM-SafetyBench - # prompt tokens",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "MM-SafetyBench - # output tokens",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "Hateful Memes - # eval",
        "description": "Multimodal hate speech detection in memes ([Kiela et al., 2020](https://arxiv.org/abs/2005.04790)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Hateful Memes"
        }
      },
      {
        "value": "Hateful Memes - # train",
        "description": "Multimodal hate speech detection in memes ([Kiela et al., 2020](https://arxiv.org/abs/2005.04790)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Hateful Memes"
        }
      },
      {
        "value": "Hateful Memes - truncated",
        "description": "Multimodal hate speech detection in memes ([Kiela et al., 2020](https://arxiv.org/abs/2005.04790)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Hateful Memes"
        }
      },
      {
        "value": "Hateful Memes - # prompt tokens",
        "description": "Multimodal hate speech detection in memes ([Kiela et al., 2020](https://arxiv.org/abs/2005.04790)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Hateful Memes"
        }
      },
      {
        "value": "Hateful Memes - # output tokens",
        "description": "Multimodal hate speech detection in memes ([Kiela et al., 2020](https://arxiv.org/abs/2005.04790)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Hateful Memes"
        }
      },
      {
        "value": "Unicorn - # eval",
        "description": "Safety Evaluation Benchmark for Evaluating on Out-of-Distribution and Sketch Images ([Tu et al., 2023](https://arxiv.org/abs/2311.16101)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Unicorn"
        }
      },
      {
        "value": "Unicorn - # train",
        "description": "Safety Evaluation Benchmark for Evaluating on Out-of-Distribution and Sketch Images ([Tu et al., 2023](https://arxiv.org/abs/2311.16101)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Unicorn"
        }
      },
      {
        "value": "Unicorn - truncated",
        "description": "Safety Evaluation Benchmark for Evaluating on Out-of-Distribution and Sketch Images ([Tu et al., 2023](https://arxiv.org/abs/2311.16101)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Unicorn"
        }
      },
      {
        "value": "Unicorn - # prompt tokens",
        "description": "Safety Evaluation Benchmark for Evaluating on Out-of-Distribution and Sketch Images ([Tu et al., 2023](https://arxiv.org/abs/2311.16101)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Unicorn"
        }
      },
      {
        "value": "Unicorn - # output tokens",
        "description": "Safety Evaluation Benchmark for Evaluating on Out-of-Distribution and Sketch Images ([Tu et al., 2023](https://arxiv.org/abs/2311.16101)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Unicorn"
        }
      },
      {
        "value": "Bingo - # eval",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Bingo"
        }
      },
      {
        "value": "Bingo - # train",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Bingo"
        }
      },
      {
        "value": "Bingo - truncated",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Bingo"
        }
      },
      {
        "value": "Bingo - # prompt tokens",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Bingo"
        }
      },
      {
        "value": "Bingo - # output tokens",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Bingo"
        }
      },
      {
        "value": "A-OKVQA (chinese) - # eval",
        "description": "Chinese Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "A-OKVQA (chinese)"
        }
      },
      {
        "value": "A-OKVQA (chinese) - # train",
        "description": "Chinese Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "A-OKVQA (chinese)"
        }
      },
      {
        "value": "A-OKVQA (chinese) - truncated",
        "description": "Chinese Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "A-OKVQA (chinese)"
        }
      },
      {
        "value": "A-OKVQA (chinese) - # prompt tokens",
        "description": "Chinese Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "A-OKVQA (chinese)"
        }
      },
      {
        "value": "A-OKVQA (chinese) - # output tokens",
        "description": "Chinese Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "A-OKVQA (chinese)"
        }
      },
      {
        "value": "A-OKVQA (hindi) - # eval",
        "description": "Hindi Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "A-OKVQA (hindi)"
        }
      },
      {
        "value": "A-OKVQA (hindi) - # train",
        "description": "Hindi Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "A-OKVQA (hindi)"
        }
      },
      {
        "value": "A-OKVQA (hindi) - truncated",
        "description": "Hindi Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "A-OKVQA (hindi)"
        }
      },
      {
        "value": "A-OKVQA (hindi) - # prompt tokens",
        "description": "Hindi Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "A-OKVQA (hindi)"
        }
      },
      {
        "value": "A-OKVQA (hindi) - # output tokens",
        "description": "Hindi Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "A-OKVQA (hindi)"
        }
      },
      {
        "value": "A-OKVQA (spanish) - # eval",
        "description": "Spanish Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "A-OKVQA (spanish)"
        }
      },
      {
        "value": "A-OKVQA (spanish) - # train",
        "description": "Spanish Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "A-OKVQA (spanish)"
        }
      },
      {
        "value": "A-OKVQA (spanish) - truncated",
        "description": "Spanish Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "A-OKVQA (spanish)"
        }
      },
      {
        "value": "A-OKVQA (spanish) - # prompt tokens",
        "description": "Spanish Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "A-OKVQA (spanish)"
        }
      },
      {
        "value": "A-OKVQA (spanish) - # output tokens",
        "description": "Spanish Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "A-OKVQA (spanish)"
        }
      },
      {
        "value": "A-OKVQA (swahili) - # eval",
        "description": "Swahili Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "A-OKVQA (swahili)"
        }
      },
      {
        "value": "A-OKVQA (swahili) - # train",
        "description": "Swahili Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "A-OKVQA (swahili)"
        }
      },
      {
        "value": "A-OKVQA (swahili) - truncated",
        "description": "Swahili Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "A-OKVQA (swahili)"
        }
      },
      {
        "value": "A-OKVQA (swahili) - # prompt tokens",
        "description": "Swahili Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "A-OKVQA (swahili)"
        }
      },
      {
        "value": "A-OKVQA (swahili) - # output tokens",
        "description": "Swahili Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "A-OKVQA (swahili)"
        }
      },
      {
        "value": "Bingo (multilinguality) - # eval",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Bingo (multilinguality)"
        }
      },
      {
        "value": "Bingo (multilinguality) - # train",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Bingo (multilinguality)"
        }
      },
      {
        "value": "Bingo (multilinguality) - truncated",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Bingo (multilinguality)"
        }
      },
      {
        "value": "Bingo (multilinguality) - # prompt tokens",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Bingo (multilinguality)"
        }
      },
      {
        "value": "Bingo (multilinguality) - # output tokens",
        "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Bingo (multilinguality)"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 33.7,
          "description": "min=33.7, mean=33.7, max=33.7, sum=33.7 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 9.666666666666666,
          "description": "min=9, mean=9.667, max=10, sum=116 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 40.97777777777777,
          "description": "min=39.333, mean=40.978, max=41.9, sum=491.733 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=12 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 44.4,
          "description": "min=44.4, mean=44.4, max=44.4, sum=88.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 1.6,
          "description": "min=1.6, mean=1.6, max=1.6, sum=3.2 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 46.4,
          "description": "min=46.4, mean=46.4, max=46.4, sum=46.4 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=180 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 83.33333333333333,
          "description": "min=36, mean=83.333, max=112, sum=1500 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=18 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 21.4,
          "description": "min=21.1, mean=21.4, max=21.7, sum=42.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,groups=vqa_base"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,groups=vqa_base"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,groups=vqa_base"
          ]
        },
        {
          "value": 44.4,
          "description": "min=44.4, mean=44.4, max=44.4, sum=88.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,groups=vqa_base"
          ]
        },
        {
          "value": 2.8,
          "description": "min=2.8, mean=2.8, max=2.8, sum=5.6 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,groups=vqa_base"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 33.7,
          "description": "min=33.7, mean=33.7, max=33.7, sum=33.7 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 9.666666666666666,
          "description": "min=9, mean=9.667, max=10, sum=116 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 40.97777777777777,
          "description": "min=39.333, mean=40.978, max=41.9, sum=491.733 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=12 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 44.4,
          "description": "min=44.4, mean=44.4, max=44.4, sum=88.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 2.8,
          "description": "min=2.8, mean=2.8, max=2.8, sum=5.6 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 46.4,
          "description": "min=46.4, mean=46.4, max=46.4, sum=46.4 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=180 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 83.33333333333333,
          "description": "min=36, mean=83.333, max=112, sum=1500 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=18 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 21.4,
          "description": "min=21.1, mean=21.4, max=21.7, sum=42.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 33.7,
          "description": "min=33.7, mean=33.7, max=33.7, sum=33.7 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 9.666666666666666,
          "description": "min=9, mean=9.667, max=10, sum=116 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 40.97777777777777,
          "description": "min=39.333, mean=40.978, max=41.9, sum=491.733 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=12 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 44.4,
          "description": "min=44.4, mean=44.4, max=44.4, sum=88.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 1.7,
          "description": "min=1.7, mean=1.7, max=1.7, sum=3.4 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 46.4,
          "description": "min=46.4, mean=46.4, max=46.4, sum=46.4 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=180 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 83.33333333333333,
          "description": "min=36, mean=83.333, max=112, sum=1500 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=18 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 21.4,
          "description": "min=21.1, mean=21.4, max=21.7, sum=42.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 33.7,
          "description": "min=33.7, mean=33.7, max=33.7, sum=33.7 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pope:model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 9.666666666666666,
          "description": "min=9, mean=9.667, max=10, sum=116 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 40.97777777777777,
          "description": "min=39.333, mean=40.978, max=41.9, sum=491.733 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=12 (12)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 44.4,
          "description": "min=44.4, mean=44.4, max=44.4, sum=88.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 1.5,
          "description": "min=1.5, mean=1.5, max=1.5, sum=3 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 46.4,
          "description": "min=46.4, mean=46.4, max=46.4, sum=46.4 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=180 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 83.33333333333333,
          "description": "min=36, mean=83.333, max=112, sum=1500 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=18 (18)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=20 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 21.4,
          "description": "min=21.1, mean=21.4, max=21.7, sum=42.8 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/core_scenarios_general_information.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/core_scenarios_general_information.json"
      }
    ],
    "name": "general_information"
  },
  {
    "title": "Fairness",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Mean win rate",
        "description": "How many models this model outperforms on average (over columns).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {}
      },
      {
        "value": "VQAv2 (AAE) - PEM (Fairness)",
        "description": "African-American English Perturbation + Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "VQAv2 (AAE)",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "A-OKVQA (AAE) - PEM (Fairness)",
        "description": "African-American English Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "A-OKVQA (AAE)",
          "perturbation": "Fairness"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.25,
          "markdown": false
        },
        {
          "value": 0.3,
          "description": "min=0.3, mean=0.3, max=0.3, sum=0.3 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.7,
          "description": "min=0.7, mean=0.7, max=0.7, sum=0.7 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-05,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.25,
          "markdown": false
        },
        {
          "value": 0.3,
          "description": "min=0.3, mean=0.3, max=0.3, sum=0.3 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.7,
          "description": "min=0.7, mean=0.7, max=0.7, sum=0.7 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.7,
          "description": "min=0.7, mean=0.7, max=0.7, sum=0.7 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-25,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.8333333333333333,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.4,
          "description": "min=0.4, mean=0.4, max=0.4, sum=0.4 (1)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=vqa_dialect"
          ]
        },
        {
          "value": 0.8,
          "description": "min=0.8, mean=0.8, max=0.8, sum=0.8 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "a_okvqa:model=custom_qwen2.5-vl-3b-instruct-ft-50,data_augmentation=dialect_prob=1.0_source=SAE_target=AAVE,groups=a_okvqa_dialect"
          ]
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/core_scenarios_fairness.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/core_scenarios_fairness.json"
      }
    ],
    "name": "fairness"
  },
  {
    "title": "Toxicity",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Mean win rate",
        "description": "How many models this model outperforms on average (over columns).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {}
      },
      {
        "value": "MM-SafetyBench - Toxic fraction",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\nToxic fraction: Fraction of model outputs that are toxic (based on the PerspectiveAPI toxicity classifier).",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Toxic fraction",
          "run_group": "MM-SafetyBench"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/core_scenarios_toxicity.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/core_scenarios_toxicity.json"
      }
    ],
    "name": "toxicity"
  },
  {
    "title": "Translate",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Mean win rate",
        "description": "How many models this model outperforms on average (over columns).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {}
      },
      {
        "value": "A-OKVQA (chinese) - PEM (Translate)",
        "description": "Chinese Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.\n- Perturbation Translate: Translate prompts to different languages.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "A-OKVQA (chinese)",
          "perturbation": "Translate"
        }
      },
      {
        "value": "A-OKVQA (hindi) - PEM (Translate)",
        "description": "Hindi Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.\n- Perturbation Translate: Translate prompts to different languages.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "A-OKVQA (hindi)",
          "perturbation": "Translate"
        }
      },
      {
        "value": "A-OKVQA (spanish) - PEM (Translate)",
        "description": "Spanish Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.\n- Perturbation Translate: Translate prompts to different languages.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "A-OKVQA (spanish)",
          "perturbation": "Translate"
        }
      },
      {
        "value": "A-OKVQA (swahili) - PEM (Translate)",
        "description": "Swahili Translation Perturbation + A-OKVQA ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.\n- Perturbation Translate: Translate prompts to different languages.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "PEM",
          "run_group": "A-OKVQA (swahili)",
          "perturbation": "Translate"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        },
        {
          "description": "No matching runs",
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/core_scenarios_translate.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/core_scenarios_translate.json"
      }
    ],
    "name": "translate"
  }
]