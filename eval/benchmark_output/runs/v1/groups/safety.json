[
  {
    "title": "Accuracy",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Mean win rate",
        "description": "How many models this model outperforms on average (over columns).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {}
      },
      {
        "value": "MM-SafetyBench - Toxic fraction",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\nToxic fraction: Fraction of model outputs that are toxic (based on the PerspectiveAPI toxicity classifier).",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Toxic fraction",
          "run_group": "MM-SafetyBench"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-100%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-100"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/safety_accuracy.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/safety_accuracy.json"
      }
    ],
    "name": "accuracy"
  },
  {
    "title": "Toxicity",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Mean win rate",
        "description": "How many models this model outperforms on average (over columns).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {}
      },
      {
        "value": "MM-SafetyBench - Toxic fraction",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\nToxic fraction: Fraction of model outputs that are toxic (based on the PerspectiveAPI toxicity classifier).",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Toxic fraction",
          "run_group": "MM-SafetyBench"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-100%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-100"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5,
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/safety_toxicity.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/safety_toxicity.json"
      }
    ],
    "name": "toxicity"
  },
  {
    "title": "General information",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Mean win rate",
        "description": "How many models this model outperforms on average (over columns).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {}
      },
      {
        "value": "MM-SafetyBench - # eval",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "MM-SafetyBench - # train",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "MM-SafetyBench - truncated",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "MM-SafetyBench - # prompt tokens",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MM-SafetyBench"
        }
      },
      {
        "value": "MM-SafetyBench - # output tokens",
        "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MM-SafetyBench"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=130 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 24.60769230769231,
          "description": "min=21.2, mean=24.608, max=28.5, sum=319.9 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        },
        {
          "value": 112.89230769230768,
          "description": "min=16.6, mean=112.892, max=235.9, sum=1467.6 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-05",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-05"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=130 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 24.60769230769231,
          "description": "min=21.2, mean=24.608, max=28.5, sum=319.9 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        },
        {
          "value": 106.30000000000001,
          "description": "min=47.4, mean=106.3, max=271.6, sum=1381.9 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-10",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-10"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-100%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 3.0,
          "description": "min=3, mean=3, max=3, sum=39 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-100"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-100"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-100"
          ]
        },
        {
          "value": 24.230769230769226,
          "description": "min=20.333, mean=24.231, max=29, sum=315.0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-100"
          ]
        },
        {
          "value": 102.38461538461537,
          "description": "min=11.333, mean=102.385, max=215.667, sum=1331.0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-100",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-100"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 3.0,
          "description": "min=3, mean=3, max=3, sum=39 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 24.230769230769226,
          "description": "min=20.333, mean=24.231, max=29, sum=315.0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        },
        {
          "value": 158.74358974358972,
          "description": "min=78, mean=158.744, max=330.667, sum=2063.667 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-25",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-25"
          ]
        }
      ],
      [
        {
          "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
          "description": "",
          "markdown": false
        },
        {
          "markdown": false
        },
        {
          "value": 3.0,
          "description": "min=3, mean=3, max=3, sum=39 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 24.230769230769226,
          "description": "min=20.333, mean=24.231, max=29, sum=315.0 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        },
        {
          "value": 136.07692307692307,
          "description": "min=6.667, mean=136.077, max=338.333, sum=1769 (13)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-50",
            "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-50"
          ]
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/safety_general_information.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/safety_general_information.json"
      }
    ],
    "name": "general_information"
  }
]