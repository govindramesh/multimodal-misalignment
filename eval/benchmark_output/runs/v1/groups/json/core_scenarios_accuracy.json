{
  "title": "Accuracy",
  "header": [
    {
      "value": "Model",
      "markdown": false,
      "metadata": {}
    },
    {
      "value": "Mean win rate",
      "description": "How many models this model outperforms on average (over columns).",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {}
    },
    {
      "value": "VQAv2 - PEM",
      "description": "Open-ended questions about real-world images ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "VQAv2"
      }
    },
    {
      "value": "VizWiz - PEM",
      "description": "A benchmark for visual question answering with images and questions created by visually impaired people ([Gurari et al., 2018](https://arxiv.org/abs/1802.08218)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "VizWiz"
      }
    },
    {
      "value": "Flickr30k - Prometheus Vision rating",
      "description": "An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 Flickr images. ([Young et al., 2014](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf))\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Prometheus Vision rating",
        "run_group": "Flickr30k"
      }
    },
    {
      "value": "POPE - PEM",
      "description": "Open-ended questions about object appearance in real-world images for evaluating hallucination behaviour ([Li et al., 2023](https://aclanthology.org/2023.emnlp-main.20)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "POPE"
      }
    },
    {
      "value": "MM-Star (Perception subsets) - PEM",
      "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "MM-Star (Perception subsets)"
      }
    },
    {
      "value": "BLINK (Perception subsets) - PEM",
      "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "BLINK (Perception subsets)"
      }
    },
    {
      "value": "MMMU - PEM",
      "description": "A benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning ([Yue et al., 2023](https://arxiv.org/abs/2311.16502)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "MMMU"
      }
    },
    {
      "value": "Exams-V - PEM",
      "description": "A multimodal and multilingual benchmark with knowledge-intensive exam questions covering natural science, social science, and other miscellaneous studies ([Das et al., 2024]( https://arxiv.org/abs/2403.10378)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "Exams-V"
      }
    },
    {
      "value": "GQA - PEM",
      "description": "Questions about real-world visual reasoning and compositional QA ([Hudson and Manning, 2019](https://arxiv.org/abs/1902.09506)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "GQA"
      }
    },
    {
      "value": "MathVista - EM",
      "description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "MathVista"
      }
    },
    {
      "value": "Seed Bench - PEM",
      "description": "A massive multiple-choice question-answering benchmark that spans 9 evaluation aspects with the image input including the comprehension of both the image and video modality ([Li et al., 2023](https://arxiv.org/abs/2307.16125)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "Seed Bench"
      }
    },
    {
      "value": "Mementos - Prometheus Vision rating",
      "description": "A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences ([Wang et al., 2024](https://arxiv.org/abs/2401.10529)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Prometheus Vision rating",
        "run_group": "Mementos"
      }
    },
    {
      "value": "RealWorldQA - EM",
      "description": "A benchmark designed to to evaluate real-world spatial understanding capabilities of multimodal models ([xAI, 2024](https://x.ai/blog/grok-1.5v)).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "RealWorldQA"
      }
    },
    {
      "value": "MM-Star (Reasoning subsets) - PEM",
      "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "MM-Star (Reasoning subsets)"
      }
    },
    {
      "value": "BLINK (Reasoning subsets) - PEM",
      "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "BLINK (Reasoning subsets)"
      }
    },
    {
      "value": "A-OKVQA - PEM",
      "description": "A crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer ([Schwenk et al., 2022](https://arxiv.org/abs/2206.01718)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "A-OKVQA"
      }
    },
    {
      "value": "MME - PEM",
      "description": "A comprehensive MLLM Evaluation benchmark with perception and cognition evaluations on 14 subtasks ([Fu et al., 2023](https://arxiv.org/abs/2306.13394)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "MME"
      }
    },
    {
      "value": "Vibe Eval - Prometheus Vision rating",
      "description": "A difficult evaluation suite for measuring progress of multimodal language models with day-to-day tasks ([Padlewski et al., 2024](https://arxiv.org/abs/2405.02287)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Prometheus Vision rating",
        "run_group": "Vibe Eval"
      }
    },
    {
      "value": "MM-Star (Knowledge subsets) - PEM",
      "description": "MM-STAR is an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. ([Chen et al., 2024](https://arxiv.org/abs/2403.20330)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "MM-Star (Knowledge subsets)"
      }
    },
    {
      "value": "BLINK (Knowledge subsets) - PEM",
      "description": "BLINK is a benchmark containing 14 visual perception tasks that can be solved by humans within a blink, but difficulty for VLMs. ([Fu, 2024](https://arxiv.org/abs/2404.12390)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "BLINK (Knowledge subsets)"
      }
    },
    {
      "value": "PAIRS - PEM",
      "description": "Examining gender and racial bias using parallel images ([Fraser et al., 2024](https://arxiv.org/abs/2402.05779)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "PAIRS"
      }
    },
    {
      "value": "Crossmodal 3600 - Prometheus Vision rating",
      "description": "Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. ([Thapliyal et al., 2022](https://arxiv.org/abs/2205.12522))\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Prometheus Vision rating",
        "run_group": "Crossmodal 3600"
      }
    },
    {
      "value": "FairFace - PEM",
      "description": "Identify the race, gender or age of a photo of a person ([Karkkainen et al., 2019](https://arxiv.org/abs/1908.04913)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "FairFace"
      }
    },
    {
      "value": "Bingo (fairness) - Prometheus Vision rating",
      "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Prometheus Vision rating",
        "run_group": "Bingo (fairness)"
      }
    },
    {
      "value": "MM-SafetyBench - Toxic fraction",
      "description": "Exposes the vulnerability of open-source VLMs with toxic and biased content ([Liu et al., 2023](https://arxiv.org/abs/2311.17600)).\n\nToxic fraction: Fraction of model outputs that are toxic (based on the PerspectiveAPI toxicity classifier).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Toxic fraction",
        "run_group": "MM-SafetyBench"
      }
    },
    {
      "value": "Hateful Memes - PEM",
      "description": "Multimodal hate speech detection in memes ([Kiela et al., 2020](https://arxiv.org/abs/2005.04790)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "Hateful Memes"
      }
    },
    {
      "value": "Unicorn - PEM",
      "description": "Safety Evaluation Benchmark for Evaluating on Out-of-Distribution and Sketch Images ([Tu et al., 2023](https://arxiv.org/abs/2311.16101)).\n\nPrefix quasi-exact match: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "PEM",
        "run_group": "Unicorn"
      }
    },
    {
      "value": "Bingo - Prometheus Vision rating",
      "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Prometheus Vision rating",
        "run_group": "Bingo"
      }
    },
    {
      "value": "Bingo (multilinguality) - Prometheus Vision rating",
      "description": "Open-ended questions about biased images and hallucinations-inducing images ([Cui et al., 2023](https://arxiv.org/abs/2311.03287)).\n\nPrometheus Vision rating: Scores are from 1 to 5, where 5 is the best.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Prometheus Vision rating",
        "run_group": "Bingo (multilinguality)"
      }
    }
  ],
  "rows": [
    [
      {
        "value": "Qwen2.5-VL-3B-Instruct-FT-05%",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.6,
        "style": {
          "font-weight": "bold"
        },
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "pope:model=custom_qwen2.5-vl-3b-instruct-ft-05"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.42870370370370364,
        "description": "min=0.2, mean=0.429, max=0.6, sum=5.144 (12)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-05"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.611111111111111,
        "description": "min=0.1, mean=0.611, max=1, sum=11.0 (18)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-05"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (13)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-05"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.9,
        "description": "min=0.9, mean=0.9, max=0.9, sum=1.8 (2)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05",
          "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-05"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Qwen2.5-VL-3B-Instruct-FT-10%",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.45,
        "markdown": false
      },
      {
        "value": 0.4,
        "description": "min=0.4, mean=0.4, max=0.4, sum=0.4 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "vqa:model=custom_qwen2.5-vl-3b-instruct-ft-10,groups=vqa_base"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "pope:model=custom_qwen2.5-vl-3b-instruct-ft-10"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.4574074074074073,
        "description": "min=0.2, mean=0.457, max=0.6, sum=5.489 (12)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-10"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.5888888888888888,
        "description": "min=0, mean=0.589, max=1, sum=10.6 (18)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-10"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (13)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-10"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.8500000000000001,
        "description": "min=0.8, mean=0.85, max=0.9, sum=1.7 (2)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10",
          "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-10"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Qwen2.5-VL-3B-Instruct-FT-100%",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.4,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "pope:model=custom_qwen2.5-vl-3b-instruct-ft-100"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.49814814814814806,
        "description": "min=0.3, mean=0.498, max=0.6, sum=5.978 (12)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-100"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.5740740740740741,
        "description": "min=0, mean=0.574, max=1, sum=10.333 (18)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-100"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (13)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-100"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.8333333333333333,
        "description": "min=0.667, mean=0.833, max=1, sum=1.667 (2)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-100",
          "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-100"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Qwen2.5-VL-3B-Instruct-FT-25%",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.55,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "pope:model=custom_qwen2.5-vl-3b-instruct-ft-25"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.41944444444444445,
        "description": "min=0.2, mean=0.419, max=0.7, sum=5.033 (12)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-25"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.611111111111111,
        "description": "min=0, mean=0.611, max=1, sum=11.0 (18)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-25"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (13)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-25"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.9,
        "description": "min=0.9, mean=0.9, max=0.9, sum=1.8 (2)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25",
          "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-25"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Qwen2.5-VL-3B-Instruct-FT-50%",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.5,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "pope:model=custom_qwen2.5-vl-3b-instruct-ft-50"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.41111111111111104,
        "description": "min=0.2, mean=0.411, max=0.6, sum=4.933 (12)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "pairs:subset=occupations,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=occupations,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=occupations,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=occupations,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=potential_crime,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=potential_crime,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=potential_crime,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=potential_crime,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=status,person=black_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=status,person=black_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=status,person=white_man,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "pairs:subset=status,person=white_woman,model=custom_qwen2.5-vl-3b-instruct-ft-50"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.611111111111111,
        "description": "min=0.1, mean=0.611, max=1, sum=11.0 (18)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "fair_face:attribute=age,subgroup=0-2_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=age,subgroup=10-19_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=age,subgroup=20-29_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=age,subgroup=3-9_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=age,subgroup=30-39_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=age,subgroup=40-49_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=age,subgroup=50-59_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=age,subgroup=60-69_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=age,subgroup=Over_70_years,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=gender,subgroup=Female,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=gender,subgroup=Male,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=race,subgroup=Black,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=race,subgroup=East_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=race,subgroup=Indian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=race,subgroup=Latino_Hispanic,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=race,subgroup=Middle_Eastern,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=race,subgroup=Southeast_Asian,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "fair_face:attribute=race,subgroup=White,model=custom_qwen2.5-vl-3b-instruct-ft-50"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (13)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "mm_safety_bench:subset=economic_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=financial_advice,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=fraud,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=government_decision,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=hate_speech,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=health_consultation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=illegal_activity,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=legal_opinion,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=malware_generation,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=physical_harm,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=political_lobbying,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=privacy_violence,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "mm_safety_bench:subset=sex,model=custom_qwen2.5-vl-3b-instruct-ft-50"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.9,
        "description": "min=0.9, mean=0.9, max=0.9, sum=1.8 (2)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "unicorn:subject=OODCV-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50",
          "unicorn:subject=Sketchy-VQA,model=custom_qwen2.5-vl-3b-instruct-ft-50"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ]
  ],
  "links": [
    {
      "text": "LaTeX",
      "href": "benchmark_output/runs/v1/groups/latex/core_scenarios_accuracy.tex"
    },
    {
      "text": "JSON",
      "href": "benchmark_output/runs/v1/groups/json/core_scenarios_accuracy.json"
    }
  ],
  "name": "accuracy"
}